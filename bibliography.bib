@incollection{LITTMAN1994157,
title = {Markov games as a framework for multi-agent reinforcement learning},
editor = {William W. Cohen and Haym Hirsh},
booktitle = {Machine Learning Proceedings 1994},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {157-163},
year = {1994},
isbn = {978-1-55860-335-6},
doi = {https://doi.org/10.1016/B978-1-55860-335-6.50027-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781558603356500271},
author = {Michael L. Littman},
abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis-tic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.}
}

@book{weiss1999multiagent,
  title={Multiagent systems: a modern approach to distributed artificial intelligence},
  author={Weiss, Gerhard},
  year={1999},
  publisher={MIT press}
}

@misc{aguzzi,
title= {Collective Learning},
author = {Gianluca Aguzzi},
year = {2021},
note= {Lesson on Collective Learning},
}

@article{bettini2022vmas,
  title = {VMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learning},
  author = {Bettini, Matteo and Kortvelesy, Ryan and Blumenkamp, Jan and Prorok, Amanda},
  year = {2022},
  journal={The 16th International Symposium on Distributed Autonomous Robotic Systems},
  publisher={Springer}
}

@ARTICLE{4445757,
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 
  title={A Comprehensive Survey of Multiagent Reinforcement Learning}, 
  year={2008},
  volume={38},
  number={2},
  pages={156-172},
  doi={10.1109/TSMCC.2007.913919}}

  @article{DBLP:journals/corr/MnihKSGAWR13,
  author       = {Volodymyr Mnih and
                  Koray Kavukcuoglu and
                  David Silver and
                  Alex Graves and
                  Ioannis Antonoglou and
                  Daan Wierstra and
                  Martin A. Riedmiller},
  title        = {Playing Atari with Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1312.5602},
  year         = {2013},
  url          = {http://arxiv.org/abs/1312.5602},
  eprinttype    = {arXiv},
  eprint       = {1312.5602},
  timestamp    = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{4421430,
  author={Dung, Le Tien and Komeda, Takashi and Takagi, Motoki},
  booktitle={SICE Annual Conference 2007}, 
  title={Reinforcement learning in non-markovian environments using automatic discovery of subgoals}, 
  year={2007},
  volume={},
  number={},
  pages={2601-2605},
  doi={10.1109/SICE.2007.4421430}}

@INPROCEEDINGS{4655239,
  author={Le Tien Dung and Takashi Komeda and Motoki Takagi},
  booktitle={2008 SICE Annual Conference}, 
  title={Efficient experience reuse in non-Markovian environments}, 
  year={2008},
  volume={},
  number={},
  pages={3327-3332},
  doi={10.1109/SICE.2008.4655239}}


@article{Banchi_2018,
	abstract = {Quantum systems interacting with an unknown environment are notoriously difficult to model, especially in presence of non-Markovian and non-perturbative effects. Here we introduce a neural network based approach, which has the mathematical simplicity of the Gorini--Kossakowski--Sudarshan--Lindblad master equation, but is able to model non-Markovian effects in different regimes. This is achieved by using recurrent neural networks (RNNs) for defining Lindblad operators that can keep track of memory effects. Building upon this framework, we also introduce a neural network architecture that is able to reproduce the entire quantum evolution, given an initial state. As an application we study how to train these models for quantum process tomography, showing that RNNs are accurate over different times and regimes.},
	author = {Leonardo Banchi and Edward Grant and Andrea Rocchetto and Simone Severini},
	doi = {10.1088/1367-2630/aaf749},
	journal = {New Journal of Physics},
	month = {dec},
	number = {12},
	pages = {123030},
	publisher = {IOP Publishing},
	title = {Modelling non-markovian quantum processes with recurrent neural networks},
	url = {https://dx.doi.org/10.1088/1367-2630/aaf749},
	volume = {20},
	year = {2018},
	bdsk-url-1 = {https://dx.doi.org/10.1088/1367-2630/aaf749}}

@inproceedings{flame,
author = {Richmond, Paul and Coakley, Simon and Romano, Daniela M.},
title = {A High Performance Agent Based Modelling Framework on Graphics Card Hardware with CUDA},
year = {2009},
isbn = {9780981738178},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We present an efficient implementation of a high performance parallel framework for Agent Based Modelling (ABM), exploiting the parallel architecture of the Graphics Processing Unit (GPU). It provides a mapping between formal agent specifications, with C based scripting, and optimised NVIDIA Compute Unified Device Architecture (CUDA) code. The mapping of agent data structures and agent communication is described, and our work is evaluated through a number of simple interacting agent examples. In contrast with an alternative, single machine CPU implementation, a speedup of up to 250 times is reported.},
booktitle = {Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {1125â€“1126},
numpages = {2},
keywords = {performance, parallel algorithms, agent based modelling, CUDA, graphics processing unit},
location = {Budapest, Hungary},
series = {AAMAS '09}
}

@article{Mnih2015,
  doi = {10.1038/nature14236},
  url = {https://doi.org/10.1038/nature14236},
  year = {2015},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {518},
  number = {7540},
  pages = {529--533},
  author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  title = {Human-level control through deep reinforcement learning},
  journal = {Nature}
}

@article{Du2020,
  doi = {10.1007/s10462-020-09938-y},
  url = {https://doi.org/10.1007/s10462-020-09938-y},
  year = {2020},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {54},
  number = {5},
  pages = {3215--3238},
  author = {Wei Du and Shifei Ding},
  title = {A survey on multi-agent deep reinforcement learning: from the perspective of challenges and applications},
  journal = {Artificial Intelligence Review}
}

@inbook{scarlib,
author = {Domini, Davide and Cavallari, Filippo and Aguzzi, Gianluca and Viroli, Mirko},
year = {2023},
month = {06},
pages = {52-70},
title = {ScaRLib: A Framework for Cooperative Many Agent Deep Reinforcement Learning in Scala},
isbn = {978-3-031-35360-4},
doi = {10.1007/978-3-031-35361-1_3}
}

@article{DBLP:journals/jos/PianiniMV13,
  author    = {Danilo Pianini and
               Sara Montagna and
               Mirko Viroli},
  title     = {Chemical-oriented simulation of computational systems with {ALCHEMIST}},
  journal   = {J. Simulation},
  volume    = {7},
  number    = {3},
  pages     = {202--215},
  year      = {2013},
  url       = {https://doi.org/10.1057/jos.2012.27},
  doi       = {10.1057/jos.2012.27},
  timestamp = {Fri, 30 Nov 2018 13:25:08 +0100},
  biburl    = {https://dblp.org/rec/journals/jos/PianiniMV13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Casadei2022,
  doi = {10.1016/j.softx.2022.101248},
  url = {https://doi.org/10.1016/j.softx.2022.101248},
  year = {2022},
  month = dec,
  publisher = {Elsevier {BV}},
  volume = {20},
  pages = {101248},
  author = {Roberto Casadei and Mirko Viroli and Gianluca Aguzzi and Danilo Pianini},
  title = {{ScaFi}: A Scala {DSL} and Toolkit for Aggregate Programming},
  journal = {{SoftwareX}}
}

@inproceedings{DBLP:conf/acsos/AguzziCV22,
  author    = {Gianluca Aguzzi and
               Roberto Casadei and
               Mirko Viroli},
  editor    = {Roberto Casadei and
               Elisabetta Di Nitto and
               Ilias Gerostathopoulos and
               Danilo Pianini and
               Ivana Dusparic and
               Timothy Wood and
               Phyllis R. Nelson and
               Evangelos Pournaras and
               Nelly Bencomo and
               Sebastian G{\"{o}}tz and
               Christian Krupitzer and
               Claudia Raibulet},
  title     = {Addressing Collective Computations Efficiency: Towards a Platform-level
               Reinforcement Learning Approach},
  booktitle = {{IEEE} International Conference on Autonomic Computing and Self-Organizing
               Systems, {ACSOS} 2022, Virtual, CA, USA, September 19-23, 2022},
  pages     = {11--20},
  publisher = {{IEEE}},
  year      = {2022},
  url       = {https://doi.org/10.1109/ACSOS55765.2022.00019},
  doi       = {10.1109/ACSOS55765.2022.00019},
  timestamp = {Wed, 16 Nov 2022 11:08:36 +0100},
  biburl    = {https://dblp.org/rec/conf/acsos/AguzziCV22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{paszke2019pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas KÃ¶pf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{qlearning,
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	date = {1992/05/01},
	date-added = {2024-02-12 17:38:42 +0100},
	date-modified = {2024-02-12 17:38:42 +0100},
	doi = {10.1007/BF00992698},
	id = {Watkins1992},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {3},
	pages = {279--292},
	title = {Q-learning},
	url = {https://doi.org/10.1007/BF00992698},
	volume = {8},
	year = {1992},
	bdsk-url-1 = {https://doi.org/10.1007/BF00992698}}

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@misc{lowe2020multiagent,
      title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}, 
      author={Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
      year={2020},
      eprint={1706.02275},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{vanhasselt2015deep,
      title={Deep Reinforcement Learning with Double Q-learning}, 
      author={Hado van Hasselt and Arthur Guez and David Silver},
      year={2015},
      eprint={1509.06461},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{schaul2016prioritized,
      title={Prioritized Experience Replay}, 
      author={Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
      year={2016},
      eprint={1511.05952},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2016dueling,
      title={Dueling Network Architectures for Deep Reinforcement Learning}, 
      author={Ziyu Wang and Tom Schaul and Matteo Hessel and Hado van Hasselt and Marc Lanctot and Nando de Freitas},
      year={2016},
      eprint={1511.06581},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{quillen2018deep,
      title={Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods}, 
      author={Deirdre Quillen and Eric Jang and Ofir Nachum and Chelsea Finn and Julian Ibarz and Sergey Levine},
      year={2018},
      eprint={1802.10264},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{mnih2013playing,
      title={Playing Atari with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{liu2023comparative,
      title={A Comparative Analysis of Deep Reinforcement Learning-enabled Freeway Decision-making for Automated Vehicles}, 
      author={Teng Liu and Yuyou Yang and Wenxuan Xiao and Xiaolin Tang and Mingzhu Yin},
      year={2023},
      eprint={2008.01302},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@article{CHEN2023110335,
	abstract = {In light of the emergence of deep reinforcement learning (DRL) in recommender systems research and several fruitful results in recent years, this survey aims to provide a timely and comprehensive overview of recent trends of deep reinforcement learning in recommender systems. We start by motivating the application of DRL in recommender systems, followed by a taxonomy of current DRL-based recommender systems and a summary of existing methods. We discuss emerging topics, open issues, and provide our perspective on advancing the domain. The survey serves as introductory material for readers from academia and industry to the topic and identifies notable opportunities for further research.},
	author = {Xiaocong Chen and Lina Yao and Julian McAuley and Guanglin Zhou and Xianzhi Wang},
	doi = {https://doi.org/10.1016/j.knosys.2023.110335},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Deep reinforcement learning, Deep learning, Recommender systems},
	pages = {110335},
	title = {Deep reinforcement learning in recommender systems: A survey and new perspectives},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705123000850},
	volume = {264},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705123000850},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2023.110335}}


@incollection{MUNDRU202339,
	abstract = {With the advent of Internet of Things in health care systems, patients are increasingly opting for personalized health care systems. Understanding what makes individuals unique (their hereditary qualities, climate, and way of life) is integral to moving the focal point of health care from treating disease to conveying the best consideration for all individuals throughout their lives. This is the guarantee of customized health care. We are at a tipping point that will assist in transforming this guarantee into a reality. Advances in science, data, examinations, calculations, and computerized innovation empower us to understand the elements that influence every person's well-being with more clarity than at any previous time in history. With these data, we can further develop care at each stage, from keeping people well to detecting and preventing illness, giving the perfect drugs at the ideal time, and continuously checking the progression of disease and the results of therapy . Further developments in this direction will change results for patients and permit the frameworks for well-being to use assets more proficiently and successfully. The heterogeneous characteristics of such diseases and conditions presents incredible difficulties for custom-fit infection across the board and tending to the needs of neglected patient. Applying novel deep learning strategies to clinical investigations into such provocative illnesses shows promising outcomes and great potential for daccurate applications of medicine in clinical exploration and practice. In this chapter, we feature on the clinical use of Deep-Q learning procedures for choices for patient therapy and other suitable aspects of medicine. The experimental results indicate better performance compared with other popular methods of deep learning applied in personalized health care.},
	author = {Yamuna Mundru and Manas Kumar Yogi and Jyotir Moy Chatterjee},
	booktitle = {Deep Learning in Personalized Healthcare and Decision Support},
	doi = {https://doi.org/10.1016/B978-0-443-19413-9.00024-2},
	editor = {Harish Garg and Jyotir Moy Chatterjee},
	isbn = {978-0-443-19413-9},
	keywords = {Deep-Q learning, Health care, IoT, Machine learning, Patient},
	pages = {39-47},
	publisher = {Academic Press},
	title = {Chapter 3 - Application of Deep-Q learning in personalized health care Internet of Things ecosystem},
	url = {https://www.sciencedirect.com/science/article/pii/B9780443194139000242},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/B9780443194139000242},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-443-19413-9.00024-2}}

@article{DBLP:journals/corr/abs-2006-16712,
  author       = {Thomas M. Moerland and
                  Joost Broekens and
                  Catholijn M. Jonker},
  title        = {Model-based Reinforcement Learning: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2006.16712},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.16712},
  eprinttype    = {arXiv},
  eprint       = {2006.16712},
  timestamp    = {Thu, 02 Jul 2020 14:42:48 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-16712.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{robotics2030122,
	abstract = {In robotics, the ultimate goal of reinforcement learning is to endow robots with the ability to learn, improve, adapt and reproduce tasks with dynamically changing constraints based on exploration and autonomous learning. We give a summary of the state-of-the-art of reinforcement learning in the context of robotics, in terms of both algorithms and policy representations. Numerous challenges faced by the policy representation in robotics are identified. Three recent examples for the application of reinforcement learning to real-world robots are described: a pancake flipping task, a bipedal walking energy minimization task and an archery-based aiming task. In all examples, a state-of-the-art expectation-maximization-based reinforcement learning is used, and different policy representations are proposed and evaluated for each task. The proposed policy representations offer viable solutions to six rarely-addressed challenges in policy representations: correlations, adaptability, multi-resolution, globality, multi-dimensionality and convergence. Both the successes and the practical difficulties encountered in these examples are discussed. Based on insights from these particular cases, conclusions are drawn about the state-of-the-art and the future perspective directions for reinforcement learning in robotics.},
	author = {Kormushev, Petar and Calinon, Sylvain and Caldwell, Darwin G.},
	doi = {10.3390/robotics2030122},
	issn = {2218-6581},
	journal = {Robotics},
	number = {3},
	pages = {122--148},
	title = {Reinforcement Learning in Robotics: Applications and Real-World Challenges},
	url = {https://www.mdpi.com/2218-6581/2/3/122},
	volume = {2},
	year = {2013},
	bdsk-url-1 = {https://www.mdpi.com/2218-6581/2/3/122},
	bdsk-url-2 = {https://doi.org/10.3390/robotics2030122}}


@inbook{inbook,
	author = {Sewak, Mohit},
	doi = {10.1007/978-981-13-8285-7_10},
	isbn = {978-981-13-8284-0},
	month = {06},
	pages = {127-140},
	title = {Policy-Based Reinforcement Learning Approaches: Stochastic Policy Gradient and the REINFORCE Algorithm},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/978-981-13-8285-7_10}}

@misc{andrychowicz2020matters,
      title={What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study}, 
      author={Marcin Andrychowicz and Anton Raichuk and Piotr StaÅ„czyk and Manu Orsini and Sertan Girgin and Raphael Marinier and LÃ©onard Hussenot and Matthieu Geist and Olivier Pietquin and Marcin Michalski and Sylvain Gelly and Olivier Bachem},
      year={2020},
      eprint={2006.05990},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{degris2013offpolicy,
      title={Off-Policy Actor-Critic}, 
      author={Thomas Degris and Martha White and Richard S. Sutton},
      year={2013},
      eprint={1205.4839},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{MARTINEZDIAZ2018275,
	abstract = {Autonomous driving is expected to revolutionize road traffic attenuating current externalities, especially accidents and congestion. Carmakers, researchers and administrations have been working on autonomous driving for years and significant progress has been made. However, the doubts and challenges to overcome are still huge, as the implementation of an autonomous driving environment encompasses not only complex automotive technology, but also human behavior, ethics, traffic management strategies, policies, liability, etc. As a result, carmakers do not expect to commercially launch fully driverless vehicles in the short-term. From the technical perspective, the unequivocal detection of obstacles at high speeds and long distances is one of the greatest difficulties to face. Regarding traffic management strategies, all approaches share the vision that vehicles should behave cooperatively. General V2V cooperation and platooning are options being discussed, both with multiple variants. Various strategies, built from different standpoints, are being designed and validated using simulation. Besides, legal issues have already been arisen in the context of highly-automated driving. They range from the need for special driving licenses to much more intricate topics like liability in the event of an accident or privacy issues. All these legal and ethical concerns could hinder the spread of autonomous vehicles once technologically feasible. This paper provides an overview of the current state of the art in the key aspects of autonomous driving. Based on the information received in situ from top research centers in the field and on a literature review, authors highlight the most important advances and findings reached so far, discuss different approaches regarding autonomous traffic and propose a framework for future research.},
	author = {Margarita Mart{\'\i}nez-D{\'\i}az and Francesc Soriguera},
	doi = {https://doi.org/10.1016/j.trpro.2018.10.103},
	issn = {2352-1465},
	journal = {Transportation Research Procedia},
	keywords = {autonomous vehicles, vehicle technology, cooperative driving, traffic efficiency, vehicle automation impacts},
	note = {XIII Conference on Transport Engineering, CIT2018},
	pages = {275-282},
	title = {Autonomous vehicles: theoretical and practical challenges},
	url = {https://www.sciencedirect.com/science/article/pii/S2352146518302606},
	volume = {33},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S2352146518302606},
	bdsk-url-2 = {https://doi.org/10.1016/j.trpro.2018.10.103}}


@article{nlp-rl,
	abstract = {In recent years some researchers have explored the use of reinforcement learning (RL) algorithms as key components in the solution of various natural language processing (NLP) tasks. For instance, some of these algorithms leveraging deep neural learning have found their way into conversational systems. This paper reviews the state of the art of RL methods for their possible use for different problems of NLP, focusing primarily on conversational systems, mainly due to their growing relevance. We provide detailed descriptions of the problems as well as discussions of why RL is well-suited to solve them. Also, we analyze the advantages and limitations of these methods. Finally, we elaborate on promising research directions in NLP that might benefit from RL.},
	author = {Uc-Cetina, V{\'\i}ctor and Navarro-Guerrero, Nicol{\'a}s and Martin-Gonzalez, Anabel and Weber, Cornelius and Wermter, Stefan},
	date = {2023/02/01},
	date-added = {2024-02-29 18:27:12 +0100},
	date-modified = {2024-02-29 18:27:12 +0100},
	doi = {10.1007/s10462-022-10205-5},
	id = {Uc-Cetina2023},
	isbn = {1573-7462},
	journal = {Artificial Intelligence Review},
	number = {2},
	pages = {1543--1575},
	title = {Survey on reinforcement learning for language processing},
	url = {https://doi.org/10.1007/s10462-022-10205-5},
	volume = {56},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1007/s10462-022-10205-5}}


@article{finance-rl,
	abstract = {Abstract The rapid changes in the finance industry due to the increasing amount of data have revolutionized the techniques on data processing and data analysis and brought new theoretical and computational challenges. In contrast to classical stochastic control theory and other analytical approaches for solving financial decision-making problems that heavily reply on model assumptions, new developments from reinforcement learning (RL) are able to make full use of the large amount of financial data with fewer model assumptions and to improve decisions in complex financial environments. This survey paper aims to review the recent developments and use of RL approaches in finance. We give an introduction to Markov decision processes, which is the setting for many of the commonly used RL approaches. Various algorithms are then introduced with a focus on value- and policy-based methods that do not require any model assumptions. Connections are made with neural networks to extend the framework to encompass deep RL algorithms. We then discuss in detail the application of these RL algorithms in a variety of decision-making problems in finance, including optimal execution, portfolio optimization, option pricing and hedging, market making, smart order routing, and robo-advising. Our survey concludes by pointing out a few possible future directions for research.},
	author = {Hambly, Ben and Xu, Renyuan and Yang, Huining},
	doi = {https://doi.org/10.1111/mafi.12382},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/mafi.12382},
	journal = {Mathematical Finance},
	number = {3},
	pages = {437-503},
	title = {Recent advances in reinforcement learning in finance},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mafi.12382},
	volume = {33},
	year = {2023},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mafi.12382},
	bdsk-url-2 = {https://doi.org/10.1111/mafi.12382}}


@article{softmax-exploration,
	author = {Vamplew, Peter and Dazeley, Richard and Foale, Cameron},
	doi = {10.1016/j.neucom.2016.09.141},
	journal = {Neurocomputing},
	month = {06},
	title = {Softmax Exploration Strategies for Multiobjective Reinforcement Learning},
	volume = {263},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1016/j.neucom.2016.09.141}}

@article{10.5555/944919.944941,
author = {Auer, Peter},
title = {Using confidence bounds for exploitation-exploration trade-offs},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We show how a standard tool from statistics --- namely confidence bounds --- can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers only O((ST)1/2) regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O(T1/2).},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {397â€“422},
numpages = {26},
keywords = {bandit problem, exploitation-exploration, linear value function, online Learning, reinforcement learning}
}

@misc{fortunato2019noisy,
      title={Noisy Networks for Exploration}, 
      author={Meire Fortunato and Mohammad Gheshlaghi Azar and Bilal Piot and Jacob Menick and Ian Osband and Alex Graves and Vlad Mnih and Remi Munos and Demis Hassabis and Olivier Pietquin and Charles Blundell and Shane Legg},
      year={2019},
      eprint={1706.10295},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{DBLP:conf/acsos/AguzziCV22,
  author       = {Gianluca Aguzzi and
                  Roberto Casadei and
                  Mirko Viroli},
  editor       = {Roberto Casadei and
                  Elisabetta Di Nitto and
                  Ilias Gerostathopoulos and
                  Danilo Pianini and
                  Ivana Dusparic and
                  Timothy Wood and
                  Phyllis R. Nelson and
                  Evangelos Pournaras and
                  Nelly Bencomo and
                  Sebastian G{\"{o}}tz and
                  Christian Krupitzer and
                  Claudia Raibulet},
  title        = {Addressing Collective Computations Efficiency: Towards a Platform-level
                  Reinforcement Learning Approach},
  booktitle    = {{IEEE} International Conference on Autonomic Computing and Self-Organizing
                  Systems, {ACSOS} 2022, Virtual, CA, USA, September 19-23, 2022},
  pages        = {11--20},
  publisher    = {{IEEE}},
  year         = {2022},
  url          = {https://doi.org/10.1109/ACSOS55765.2022.00019},
  doi          = {10.1109/ACSOS55765.2022.00019},
  timestamp    = {Wed, 16 Nov 2022 11:08:36 +0100},
  biburl       = {https://dblp.org/rec/conf/acsos/AguzziCV22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}